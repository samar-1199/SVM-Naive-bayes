{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Q1.**What is a Support Vector Machine (SVM)"
      ],
      "metadata": {
        "id": "HK3PWXldJQ-P"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "-  Support vector machines (SVMs) are powerful yet flexible supervised machine learning algorithm which is used for both classification and regression. But generally, they are used in classification problems"
      ],
      "metadata": {
        "id": "kHlNTReJJnut"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q2.**What is the difference between Hard Margin and Soft Margin SVM"
      ],
      "metadata": {
        "id": "sBe_374HJok8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "-  In a hard margin SVM, the objective is to identify a hyperplane that completely separates data points belonging to different classes, ensuring a clear demarcation with the utmost margin width possible. This margin is the distance between the hyperplane and the nearest data point, also known as the support vectors.\n",
        "\n",
        "-  Soft margin SVM allows for some margin violations, meaning that it permits certain data points to fall within the margin or even on the wrong side of the decision boundary. This adaptability is managed by a factor called C, also called the \"regularization parameter,\" which helps find a balance between making the gap as big as possible and reducing mistakes in grouping things."
      ],
      "metadata": {
        "id": "OPMYVuVqJxpU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q3.**What is the mathematical intuition behind SVM"
      ],
      "metadata": {
        "id": "6qO6rV8YJyDO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "-  "
      ],
      "metadata": {
        "id": "6BOerGpyJ5LN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q4.**What is the role of Lagrange Multipliers in SVM"
      ],
      "metadata": {
        "id": "bYUwqNkfJ6BD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "-  "
      ],
      "metadata": {
        "id": "JWsf2cSBKAGt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q5.** What are Support Vectors in SVM"
      ],
      "metadata": {
        "id": "1N3qeeu9KArR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "-  Support Vector Machine (SVM) is a supervised machine learning algorithm used for classification and regression tasks. While it can handle regression problems, SVM is particularly well-suited for classification tasks."
      ],
      "metadata": {
        "id": "QjvFN4_2KHMB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q6.**What is a Support Vector Classifier (SVC)"
      ],
      "metadata": {
        "id": "khr5lUUtKHmV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "-  A Support Vector Classifier (SVC) is a type of supervised machine learning algorithm used for classification tasks. It is based on the principles of Support Vector Machines (SVMs) and works by finding the optimal boundary (or hyperplane) that best separates data points from different classes."
      ],
      "metadata": {
        "id": "H-EHmKStKNgf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q7.**What is a Support Vector Regressor (SVR)"
      ],
      "metadata": {
        "id": "aACp-BreKOG8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "-   Support Vector Regressor (SVR) is a type of Support Vector Machine (SVM) that is used for regression tasks instead of classification. Like Support Vector Classification (SVC), SVR uses many of the same principles—such as maximizing margins and using kernels—but adapts them for predicting continuous numerical values rather than class labels."
      ],
      "metadata": {
        "id": "OxfDCGAMKUAJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q8.** What is the Kernel Trick in SVM"
      ],
      "metadata": {
        "id": "vvqvjYQQKUX-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "-  The Kernel Trick in Support Vector Machines (SVMs) is a powerful method that allows SVMs to learn complex, non-linear decision boundaries without explicitly transforming the data into higher dimensions.\n",
        "\n"
      ],
      "metadata": {
        "id": "w9b004KAKaVU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q9.** Compare Linear Kernel, Polynomial Kernel, and RBF Kernel"
      ],
      "metadata": {
        "id": "92WV9g6rKatx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "-  Linear Kernel:\n",
        "\n",
        "When data is linearly separable or nearly so.\n",
        "\n",
        "Fast and interpretable.\n",
        "-  Polynomial Kernel:\n",
        "\n",
        "When relationships between features are moderately non-linear.\n",
        "\n",
        "You want a tunable degree of non-linearity (degree parameter).\n",
        "\n",
        " -  RBF Kernel:\n",
        "\n",
        "Default choice when you have no idea of the data’s structure.\n",
        "\n",
        "sually performs well if tuned properly.\n",
        "\n"
      ],
      "metadata": {
        "id": "Uw1lOHKHKgRg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q10.**What is the effect of the C parameter in SVM"
      ],
      "metadata": {
        "id": "VNt4XucRKgw3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "-  The C parameter in Support Vector Machines (SVM) controls the trade-off between model simplicity and training accuracy."
      ],
      "metadata": {
        "id": "CeZv9J5cKlzN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q11.** What is the role of the Gamma parameter in RBF Kernel SVM"
      ],
      "metadata": {
        "id": "wWzwmmSYKmU_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "-  The gamma parameter in an RBF (Radial Basis Function) kernel SVM controls how far the influence of a single training example reaches — essentially, it defines the shape and flexibility of the decision boundary."
      ],
      "metadata": {
        "id": "NseGuZ3IKrq9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q12.**What is the Naïve Bayes classifier, and why is it called \"Naïve\""
      ],
      "metadata": {
        "id": "ZWUnq6-jKsNA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "-  The Naïve Bayes classifier is a probabilistic machine learning algorithm based on Bayes' Theorem, used for classification tasks. It is especially popular for text classification (e.g., spam detection, sentiment analysis) due to its simplicity and effectiveness."
      ],
      "metadata": {
        "id": "3pA2k2AUKz7C"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q13.**What is Bayes’ Theorem"
      ],
      "metadata": {
        "id": "PBQb64vvK0WV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "-  Bayes’ Theorem is a fundamental concept in probability theory and statistics that describes how to update the probability of a hypothesis based on new evidence. It's the foundation of Bayesian inference and plays a central role in algorithms like the Naïve Bayes classifier."
      ],
      "metadata": {
        "id": "CYNIKiB-K6LS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q14.**Explain the differences between Gaussian Naïve Bayes, Multinomial Naïve Bayes, and Bernoulli Naïve Bayes"
      ],
      "metadata": {
        "id": "sDloE7Y-K6mU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "-  Gaussian Naïve Bayes:\n",
        "\n",
        "  Gaussian Naive Bayes is a type of Naive Bayes method working on continuous attributes and the data features that follows Gaussian distribution throughout the dataset.\n",
        "-  Multinomial Naive Bayes is one of the variation of Naive Bayes algorithm. A classification algorithm based on Bayes' Theorem ideal for discrete data and is typically used in text classification problems.\n",
        "\n",
        "-  Bernoulli Naive Bayes is a subcategory of the Naive Bayes Algorithm. It is typically used when the data is binary and it models the occurrence of features using Bernoulli distribution.\n",
        "\n"
      ],
      "metadata": {
        "id": "MHrZK4PfLA7s"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q15.**When should you use Gaussian Naïve Bayes over other variants"
      ],
      "metadata": {
        "id": "2zoEybt4LBSn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "-  Features are continuous and you believe they follow a Gaussian distribution.\n",
        "-  Your dataset is small to moderate in size and doesn’t require complex modeling.\n",
        "\n",
        "-  You don't have significant correlations between features (independence assumption holds).\n",
        "\n"
      ],
      "metadata": {
        "id": "eW0VBLJDLOpU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q16.** What are the key assumptions made by Naïve Bayes"
      ],
      "metadata": {
        "id": "ruKiTyRkLPGw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "-  NB requires minimal feature engineering and training time, making it ideal for applications requiring fast predictions and quick adaptation to new data.\n",
        "-  Flexibility: NB works well with both multinomial and Bernoulli word representations, adapting to different text characteristics. Multinomial captures word frequency within a document, while Bernoulli considers mere presence or absence.\n",
        "-  Naivety Assumption: The \"naive\" aspect lies in its assumption that word occurrences are independent of each other within a class. While this assumption rarely holds perfectly true, it surprisingly leads to surprisingly strong performance in many real-world scenarios."
      ],
      "metadata": {
        "id": "NSJBe_QzLdSo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q17.** What are the advantages and disadvantages of Naïve Bayes"
      ],
      "metadata": {
        "id": "l6M2PGIQLdse"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "-  Advantages:\n",
        "\n",
        " Simplicity and Speed\n",
        " Works Well with High-Dimensional Data\n",
        " Works Well with Small Datasets\n",
        "\n",
        " -  Disadvantages\n",
        "\n",
        " Strong Independence Assumption\n",
        " Poor Performance with Highly Correlated Features\n",
        " Sensitivity to Imbalanced Data"
      ],
      "metadata": {
        "id": "sB1tCKoKLkvc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q18.** Why is Naïve Bayes a good choice for text classification"
      ],
      "metadata": {
        "id": "To9qzyUoLlIa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "-  Naïve Bayes is a very effective choice for text classification, especially for problems like spam detection, sentiment analysis, and document categorization."
      ],
      "metadata": {
        "id": "TFDaKEH0LwQr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q19.** Compare SVM and Naïve Bayes for classification tasks"
      ],
      "metadata": {
        "id": "hRdCJIbWLwui"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "-  Comparing Support Vector Machines (SVM) and Naïve Bayes helps highlight their strengths, weaknesses, and ideal use cases."
      ],
      "metadata": {
        "id": "r_-g01uvL2e8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q20.**How does Laplace Smoothing help in Naïve Bayes?"
      ],
      "metadata": {
        "id": "Wo_ZRTBYL22m"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "-  Laplace smoothing (also known as add-one smoothing) helps prevent zero probabilities in Naïve Bayes classifiers, which can severely impact predictions. It's a simple but crucial technique for improving model robustness.\n",
        "\n"
      ],
      "metadata": {
        "id": "b1DmKE3tL9wH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#PRACTICAL QUESTIONS"
      ],
      "metadata": {
        "id": "gGCPC9tkL-XE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q1.**Write a Python program to train an SVM Classifier on the Iris dataset and evaluate accuracy"
      ],
      "metadata": {
        "id": "sTYBvEmQMDRZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "from sklearn import datasets\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = datasets.load_iris()\n",
        "X = iris.data  # Features\n",
        "y = iris.target  # Labels\n",
        "\n",
        "# Split the dataset into training and testing sets (80% train, 20% test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize the SVM classifier\n",
        "svm_classifier = SVC(kernel='linear')  # You can try other kernels like 'rbf', 'poly', etc.\n",
        "\n",
        "# Train the classifier\n",
        "svm_classifier.fit(X_train, y_train)\n",
        "\n",
        "# Predict on the test set\n",
        "y_pred = svm_classifier.predict(X_test)\n",
        "\n",
        "# Evaluate accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Accuracy of SVM classifier on Iris dataset: {accuracy:.2f}\")\n"
      ],
      "metadata": {
        "id": "dDGoMMEpLKKp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q2.** Write a Python program to train two SVM classifiers with Linear and RBF kernels on the Wine dataset, then\n",
        "compare their accuracies"
      ],
      "metadata": {
        "id": "gu0OoS83MQjC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "from sklearn import datasets\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the Wine dataset\n",
        "wine = datasets.load_wine()\n",
        "X = wine.data\n",
        "y = wine.target\n",
        "\n",
        "# Split into training and test sets (80% train, 20% test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize SVM classifiers with linear and RBF kernels\n",
        "svm_linear = SVC(kernel='linear', random_state=42)\n",
        "svm_rbf = SVC(kernel='rbf', random_state=42)\n",
        "\n",
        "# Train the classifiers\n",
        "svm_linear.fit(X_train, y_train)\n",
        "svm_rbf.fit(X_train, y_train)\n",
        "\n",
        "# Predict with both classifiers\n",
        "y_pred_linear = svm_linear.predict(X_test)\n",
        "y_pred_rbf = svm_rbf.predict(X_test)\n",
        "\n",
        "# Compute and print accuracies\n",
        "accuracy_linear = accuracy_score(y_test, y_pred_linear)\n",
        "accuracy_rbf = accuracy_score(y_test, y_pred_rbf)\n",
        "\n",
        "print(f\"Accuracy with Linear Kernel: {accuracy_linear:.2f}\")\n",
        "print(f\"Accuracy with RBF Kernel:    {accuracy_r_\n"
      ],
      "metadata": {
        "id": "YuLtWyZuMX69"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q3.** Write a Python program to train an SVM Regressor (SVR) on a housing dataset and evaluate it using Mean\n",
        "Squared Error (MSE)"
      ],
      "metadata": {
        "id": "viKe2UPjMYaJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVR\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Load the California Housing dataset\n",
        "housing = fetch_california_housing()\n",
        "X = housing.data\n",
        "y = housing.target\n",
        "\n",
        "# Standardize features (important for SVR)\n",
        "scaler_X = StandardScaler()\n",
        "scaler_y = StandardScaler()\n",
        "\n",
        "X_scaled = scaler_X.fit_transform(X)\n",
        "y_scaled = scaler_y.fit_transform(y.reshape(-1, 1)).ravel()\n",
        "\n",
        "# Split into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y_scaled, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize and train SVR model\n",
        "svr = SVR(kernel='rbf')  # You can try 'linear' or 'poly' kernels too\n",
        "svr.fit(X_train, y_train)\n",
        "\n",
        "# Predict on test set\n",
        "y_pred_scaled = svr.predict(X_test)\n",
        "\n",
        "# Convert scaled predictions back to original scale\n",
        "y_pred = scaler_y.inverse_transform(y_pred_scaled.reshape(-1, 1)).ravel()\n",
        "y_test_original = scaler_y.inverse_transform(y_test.reshape(-1, 1)).ravel()\n",
        "\n",
        "# Evaluate using Mean Squared Error\n",
        "mse = mean_squared_error(y_test_original, y_pred)\n",
        "print(f\"Mean Squared Error (SVR on Housing Data): {mse:.2f}\")\n"
      ],
      "metadata": {
        "id": "d1fDN_7MM1I3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q4.**Write a Python program to train an SVM Classifier with a Polynomial Kernel and visualize the decision\n",
        "boundary:"
      ],
      "metadata": {
        "id": "nF81_FIAM1yN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.pipeline import make_pipeline\n",
        "\n",
        "# Step 1: Generate a simple 2D classification dataset\n",
        "X, y = make_classification(n_samples=200, n_features=2, n_redundant=0,\n",
        "                           n_informative=2, n_clusters_per_class=1,\n",
        "                           class_sep=1.0, random_state=42)\n",
        "\n",
        "# Step 2: Create an SVM classifier with polynomial kernel\n",
        "svm_poly = make_pipeline(StandardScaler(), SVC(kernel='poly', degree=3, C=1.0, coef0=1))\n",
        "\n",
        "# Step 3: Train the classifier\n",
        "svm_poly.fit(X, y)\n",
        "\n",
        "# Step 4: Function to plot the decision boundary\n",
        "def plot_decision_boundary(clf, X, y, title):\n",
        "    # Create a mesh grid\n",
        "    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
        "    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
        "    xx, yy = np.meshgrid(np.linspace(x_min, x_max, 500),\n",
        "                         np.linspace(y_min, y_max, 500))\n",
        "    Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])\n",
        "    Z = Z.reshape(xx.shape)\n",
        "\n",
        "    # Plotting\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    plt.contourf(xx, yy, Z, alpha=0.3, cmap=plt.cm.coolwarm)\n",
        "    plt.scatter(X[:, 0], X[:, 1], c=y, cmap=plt.cm.coolwarm, edgecolors='k')\n",
        "    plt.title(title)\n",
        "    plt.xlabel(\"Feature 1\")\n",
        "    plt.ylabel(\"Feature 2\")\n",
        "    plt.grid(True)\n",
        "    plt.show()\n",
        "\n",
        "# Step 5: Visualize the decision boundary\n",
        "plot_decision_boundary(svm_poly, X, y, \"SVM with Polynomial Kernel (degree=3)\")\n"
      ],
      "metadata": {
        "id": "NdC3g3cJM7Pc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q5.**Write a Python program to train a Gaussian Naïve Bayes classifier on the Breast Cancer dataset and\n",
        "evaluate accuracy"
      ],
      "metadata": {
        "id": "8eueIcuzM7x1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Step 1: Load the Breast Cancer dataset\n",
        "data = load_breast_cancer()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Step 2: Split the data (80% train, 20% test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Step 3: Initialize and train the Gaussian Naïve Bayes classifier\n",
        "gnb = GaussianNB()\n",
        "gnb.fit(X_train, y_train)\n",
        "\n",
        "# Step 4: Predict on the test set\n",
        "y_pred = gnb.predict(X_test)\n",
        "\n",
        "# Step 5: Evaluate accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Accuracy of Gaussian Naïve Bayes on Breast Cancer dataset: {accuracy:.2f}\")\n"
      ],
      "metadata": {
        "id": "EG6607ncNCHf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q6.**Write a Python program to train a Multinomial Naïve Bayes classifier for text classification using the 20\n",
        "Newsgroups dataset"
      ],
      "metadata": {
        "id": "wD5xMIaXNCvy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import fetch_20newsgroups\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "# Step 1: Load the 20 Newsgroups dataset (use a subset or all categories)\n",
        "categories = ['sci.space', 'rec.sport.baseball', 'comp.graphics', 'talk.politics.misc']\n",
        "newsgroups = fetch_20newsgroups(subset='all', categories=categories, remove=('headers', 'footers', 'quotes'))\n",
        "\n",
        "# Step 2: Convert the text data to TF-IDF feature vectors\n",
        "vectorizer = TfidfVectorizer(stop_words='english', max_df=0.5)\n",
        "X = vectorizer.fit_transform(newsgroups.data)\n",
        "y = newsgroups.target\n",
        "\n",
        "# Step 3: Split into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Step 4: Train the Multinomial Naïve Bayes classifier\n",
        "nb_classifier = MultinomialNB()\n",
        "nb_classifier.fit(X_train, y_train)\n",
        "\n",
        "# Step 5: Predict on the test set\n",
        "y_pred = nb_classifier.predict(X_test)\n",
        "\n",
        "# Step 6: Evaluate the classifier\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Accuracy: {accuracy:.2f}\")\n",
        "print(\"\\nClassification Report:\\n\")\n",
        "print(classification_report(y_test, y_pred, target_names=newsgroups.target_names))\n"
      ],
      "metadata": {
        "id": "PvQ0V4dzNKBO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q7.** Write a Python program to train an SVM Classifier with different C values and compare the decision\n",
        "boundaries visually"
      ],
      "metadata": {
        "id": "YraTW6YRNKii"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.pipeline import make_pipeline\n",
        "\n",
        "# Generate a simple 2D classification dataset\n",
        "X, y = make_classification(n_samples=200, n_features=2, n_redundant=0,\n",
        "                           n_informative=2, n_clusters_per_class=1,\n",
        "                           class_sep=0.8, random_state=42)\n",
        "\n",
        "# List of C values to try\n",
        "C_values = [0.01, 1, 100]\n",
        "\n",
        "# Plotting setup\n",
        "fig, axes = plt.subplots(1, len(C_values), figsize=(15, 4))\n",
        "\n",
        "# Function to plot decision boundary\n",
        "def plot_decision_boundary(clf, X, y, ax, title):\n",
        "    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
        "    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
        "    xx, yy = np.meshgrid(np.linspace(x_min, x_max, 500),\n",
        "                         np.linspace(y_min, y_max, 500))\n",
        "    Z = clf.predict(np.c_[xx.ravel(), yy.ravel()]).reshape(xx.shape)\n",
        "\n",
        "    ax.contourf(xx, yy, Z, alpha=0.3, cmap=plt.cm.coolwarm)\n",
        "    ax.scatter(X[:, 0], X[:, 1], c=y, cmap=plt.cm.coolwarm, edgecolors='k')\n",
        "    ax.set_title(title)\n",
        "    ax.set_xlabel(\"Feature 1\")\n",
        "    ax.set_ylabel(\"Feature 2\")\n",
        "    ax.grid(True)\n",
        "\n",
        "# Train and plot for each C\n",
        "for i, C in enumerate(C_values):\n",
        "    svm = make_pipeline(StandardScaler(), SVC(kernel='linear', C=C))\n",
        "    svm.fit(X, y)\n",
        "    plot_decision_boundary(svm, X, y, axes[i], f\"SVM with C={C}\")\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "3OydVIUiNSlZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q8.** Write a Python program to train a Bernoulli Naïve Bayes classifier for binary classification on a dataset with\n",
        "binary features="
      ],
      "metadata": {
        "id": "4kG49IJvNTYa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.naive_bayes import BernoulliNB\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "# Step 1: Generate a binary classification dataset with binary features\n",
        "X, y = make_classification(n_samples=1000, n_features=20, n_informative=10,\n",
        "                           n_redundant=0, n_classes=2, random_state=42)\n",
        "\n",
        "# Step 2: Binarize the features (convert to 0/1)\n",
        "X_binary = (X > 0).astype(int)\n",
        "\n",
        "# Step 3: Split the dataset into training and test sets\n",
        "X_train, X_test, y_train, y_test\n"
      ],
      "metadata": {
        "id": "aXsKqE14NbEU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q9.**Write a Python program to apply feature scaling before training an SVM model and compare results with\n",
        "unscaled data"
      ],
      "metadata": {
        "id": "iUsEvjEtNbgw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Step 1: Load dataset\n",
        "data = load_breast_cancer()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Step 2: Split dataset into train/test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Step 3a: Train SVM without scaling\n",
        "svm_unscaled = SVC(kernel='rbf')\n",
        "svm_unscaled.fit(X_train, y_train)\n",
        "y_pred_unscaled = svm_unscaled.predict(X_test)\n",
        "accuracy_unscaled = accuracy_score(y_test, y_pred_unscaled)\n",
        "\n",
        "# Step 3b: Scale the data\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Step 4: Train SVM with scaled data\n",
        "svm_scaled = SVC(kernel='rbf')\n",
        "svm_scaled.fit(X_train_scaled, y_train)\n",
        "y_pred_scaled = svm_scaled.predict(X_test_scaled)\n",
        "accuracy_scaled = accuracy_score(y_test, y_pred_scaled)\n",
        "\n",
        "# Step 5: Compare results\n",
        "print(f\"Accuracy without scaling: {accuracy_unscaled:.2f}\")\n",
        "print(f\"Accuracy with scaling:    {accuracy_scaled:.2f}\")\n"
      ],
      "metadata": {
        "id": "Vya9HZDgNkmM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q10.**Write a Python program to train a Gaussian Naïve Bayes model and compare the predictions before and\n",
        "after Laplace Smoothing"
      ],
      "metadata": {
        "id": "637Csy3iNk_n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "# Step 1: Load the Iris dataset\n",
        "data = load_iris()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Step 2: Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Step 3: Train Gaussian Naïve Bayes model without Laplace smoothing (no var_smoothing)\n",
        "gnb_no_smoothing = GaussianNB()\n",
        "gnb_no_smoothing.fit(X_train, y_train)\n",
        "\n",
        "# Step 4: Predict on test set without smoothing\n",
        "y_pred_no_smoothing = gnb_no_smoothing.predict(X_test)\n",
        "accuracy_no_smoothing = accuracy_score(y_test, y_pred_no_smoothing)\n",
        "\n",
        "# Step 5: Train Gaussian Naïve Bayes model with Laplace smoothing (using var_smoothing)\n",
        "gnb_with_smoothing = GaussianNB(var_smoothing=1e-9)  # Small value for Laplace smoothing\n",
        "gnb_with_smoothing.fit(X_train, y_train)\n",
        "\n",
        "# Step 6: Predict on test set with smoothing\n",
        "y_pred_with_smoothing = gnb_with_smoothing.predict(X_test)\n",
        "accuracy_with_smoothing = accuracy_score(y_test, y_pred_with_smoothing)\n",
        "\n",
        "# Step 7: Compare the predictions and accuracy\n",
        "print(f\"Accuracy without Laplace smoothing: {accuracy_no_smoothing:.2f}\")\n",
        "print(f\"Accuracy with Laplace smoothing: {accuracy_with_smoothing:.2f}\")\n",
        "\n",
        "# Optional: Print the classification report for more insights\n",
        "print(\"\\nClassification Report without Laplace Smoothing:\\n\")\n",
        "print(classification_report(y_test, y_pred_no_smoothing))\n",
        "\n",
        "print(\"\\nClassification Report with Laplace Smoothing:\\n\")\n",
        "print(classification_report(y_test, y_pred_with_smoothing))\n"
      ],
      "metadata": {
        "id": "uJbgC4mvNuWE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q11.**Write a Python program to train an SVM Classifier and use GridSearchCV to tune the hyperparameters (C,\n",
        "gamma, kernel)"
      ],
      "metadata": {
        "id": "K0iL-k4LNvKy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Step 1: Load the Iris dataset\n",
        "data = load_iris()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Step 2: Split dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Step 3: Define the SVM model\n",
        "svm = SVC()\n",
        "\n",
        "# Step 4: Define the hyperparameters to tune using GridSearchCV\n",
        "param_grid = {\n",
        "    'C': [0.1, 1, 10, 100],  # Regularization parameter\n",
        "    'gamma': ['scale', 'auto', 0.1, 1],  # Kernel coefficient\n",
        "    'kernel': ['linear', 'rbf', 'poly']  # Type of kernel to use\n",
        "}\n",
        "\n",
        "# Step 5: Perform GridSearchCV for hyperparameter tuning\n",
        "grid_search = GridSearchCV(svm, param_grid, cv=5, scoring='accuracy', verbose=1)\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Step 6: Best hyperparameters from GridSearchCV\n",
        "print(\"Best hyperparameters found by GridSearchCV:\")\n",
        "print(grid_search.best_params_)\n",
        "\n",
        "# Step 7: Evaluate the tuned model on the test set\n",
        "best_svm = grid_search.best_estimator_\n",
        "y_pred = best_svm.predict(X_test)\n",
        "\n",
        "# Step 8: Evaluate accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Accuracy of the tuned SVM model: {accuracy:.2f}\")\n"
      ],
      "metadata": {
        "id": "RlfjgcoLN2Eq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q12.**Write a Python program to train an SVM Classifier on an imbalanced dataset and apply class weighting and\n",
        "check it improve accuracy"
      ],
      "metadata": {
        "id": "OZndg0zAN2ra"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "\n",
        "# Step 1: Load the Iris dataset\n",
        "data = load_iris()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Step 2: Make the dataset imbalanced by removing some samples from one class\n",
        "# Here we will create an imbalance by removing some of class 0\n",
        "X_imbalanced = X[y != 0]\n",
        "y_imbalanced = y[y != 0]\n",
        "\n",
        "# Step 3: Split dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_imbalanced, y_imbalanced, test_size=0.3, random_state=42)\n",
        "\n",
        "# Step 4: Compute class weights\n",
        "class_weights = compute_class_weight('balanced', classes=np.unique(y_train), y=y_train)\n",
        "class_weight_dict = dict(zip(np.unique(y_train), class_weights))\n",
        "\n",
        "# Step 5: Train an SVM classifier with class weights\n",
        "svm = SVC(kernel='rbf', class_weight=class_weight_dict)\n",
        "svm.fit(X_train, y_train)\n",
        "\n",
        "# Step 6: Predict on the test set\n",
        "y_pred = svm.predict(X_test)\n",
        "\n",
        "# Step 7: Evaluate accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Accuracy with class weights: {accuracy:.2f}\")\n",
        "\n",
        "# Step 8: Train SVM without class weights (to compare)\n",
        "svm_no_weight = SVC(kernel='rbf')\n",
        "svm_no_weight.fit(X_train, y_train)\n",
        "\n",
        "# Step 9: Predict and evaluate accuracy without class weights\n",
        "y_pred_no_weight = svm_no_weight.predict(X_test)\n",
        "accuracy_no_weight = accuracy_score(y_test, y_pred_no_weight)\n",
        "print(f\"Accuracy without class weights: {accuracy_no_weight:.2f}\")\n"
      ],
      "metadata": {
        "id": "kIF0lIckN9Ye"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q13.**Write a Python program to implement a Naïve Bayes classifier for spam detection using email data"
      ],
      "metadata": {
        "id": "_tmUkpBeOAsq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "# Step 1: Load the SMS Spam Collection dataset\n",
        "# You can download it from: https://archive.ics.uci.edu/ml/datasets/SMS+Spam+Collection\n",
        "# The dataset contains two columns: \"label\" (ham/spam) and \"message\" (the SMS content)\n",
        "url = 'https://archive.ics.uci.edu/ml/machine-learning-databases/00228/smsspamcollection.zip'\n",
        "\n",
        "# Load the data into a pandas DataFrame\n",
        "data = pd.read_csv(url, sep='\\t', header=None, names=[\"label\", \"message\"])\n",
        "\n",
        "# Step 2: Preprocess the data\n",
        "# Encode labels: ham -> 0, spam -> 1\n",
        "data['label'] = data['label'].map({'ham': 0, 'spam': 1})\n",
        "\n",
        "# Step 3: Split the dataset into features and labels\n",
        "X = data['message']  # Features (SMS text)\n",
        "y = data['label']    # Labels (ham or spam)\n",
        "\n",
        "# Step 4: Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Step 5: Convert text data into TF-IDF features\n",
        "vectorizer = TfidfVectorizer(stop_words='english', max_df=0.95)\n",
        "X_train_tfidf = vectorizer.fit_transform(X_train)\n",
        "X_test_tfidf = vectorizer.transform(X_test)\n",
        "\n",
        "# Step 6: Train the Naive Bayes classifier\n",
        "nb_classifier = MultinomialNB()\n",
        "nb_classifier.fit(X_train_tfidf, y_train)\n",
        "\n",
        "# Step 7: Make predictions on the test set\n",
        "y_pred = nb_classifier.predict(X_test_tfidf)\n",
        "\n",
        "# Step 8: Evaluate the model\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Accuracy: {accuracy:.2f}\")\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(y_test, y_pred, target_names=[\"ham\", \"spam\"]))\n"
      ],
      "metadata": {
        "id": "RqFMnKC-OINb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q14.**Write a Python program to train an SVM Classifier and a Naïve Bayes Classifier on the same dataset and\n",
        "compare their accuracy"
      ],
      "metadata": {
        "id": "3QnS4A_aOIq6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Step 1: Load the Iris dataset\n",
        "data = load_iris()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Step 2: Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Step 3: Train SVM Classifier\n",
        "svm_classifier = SVC(kernel='rbf')\n",
        "svm_classifier.fit(X_train, y_train)\n",
        "\n",
        "# Step 4: Train Naive Bayes Classifier\n",
        "nb_classifier = GaussianNB()\n",
        "nb_classifier.fit(X_train, y_train)\n",
        "\n",
        "# Step 5: Make predictions using both classifiers\n",
        "svm_predictions = svm_classifier.predict(X_test)\n",
        "nb_predictions = nb_classifier.predict(X_test)\n",
        "\n",
        "# Step 6: Calculate accuracy for both classifiers\n",
        "svm_accuracy = accuracy_score(y_test, svm_predictions)\n",
        "nb_accuracy = accuracy_score(y_test, nb_predictions)\n",
        "\n",
        "# Step 7: Compare and print the results\n",
        "print(f\"Accuracy of SVM Classifier: {svm_accuracy:.2f}\")\n",
        "print(f\"Accuracy of Naive Bayes Classifier: {nb_accuracy:.2f}\")\n"
      ],
      "metadata": {
        "id": "14QXKMIUOOW-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q15.**Write a Python program to perform feature selection before training a Naïve Bayes classifier and compare\n",
        "results"
      ],
      "metadata": {
        "id": "iTotdv86OO9z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.feature_selection import SelectKBest, f_classif\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Step 1: Load the Iris dataset\n",
        "data = load_iris()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Step 2: Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Step 3: Train Naive Bayes Classifier without feature selection\n",
        "nb_classifier_no_fs = GaussianNB()\n",
        "nb_classifier_no_fs.fit(X_train, y_train)\n",
        "\n",
        "# Step 4: Make predictions and evaluate accuracy without feature selection\n",
        "y_pred_no_fs = nb_classifier_no_fs.predict(X_test)\n",
        "accuracy_no_fs = accuracy_score(y_test, y_pred_no_fs)\n",
        "\n",
        "# Step 5: Perform feature selection using SelectKBest with f_classif (ANOVA F-value)\n",
        "# Here, we will select the top 2 features based on the f_classif scoring function\n",
        "selector = SelectKBest(score_func=f_classif, k=2)\n",
        "X_train_selected = selector.fit_transform(X_train, y_train)\n",
        "X_test_selected = selector.transform(X_test)\n",
        "\n",
        "# Step 6: Train Naive Bayes Classifier with selected features\n",
        "nb_classifier_with_fs = GaussianNB()\n",
        "nb_classifier_with_fs.fit(X_train_selected, y_train)\n",
        "\n",
        "# Step 7: Make predictions and evaluate accuracy with feature selection\n",
        "y_pred_with_fs = nb_classifier_with_fs.predict(X_test_selected)\n",
        "accuracy_with_fs = accuracy_score(y_test, y_pred_with_fs)\n",
        "\n",
        "# Step 8: Compare the results\n",
        "print(f\"Accuracy without feature selection: {accuracy_no_fs:.2f}\")\n",
        "print(f\"Accuracy with feature selection: {accuracy_with_fs:.2f}\")\n"
      ],
      "metadata": {
        "id": "Is_xPM0MOWRU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q16.**Write a Python program to train an SVM Classifier using One-vs-Rest (OvR) and One-vs-One (OvO)\n",
        "strategies on the Wine dataset and compare their accuracy"
      ],
      "metadata": {
        "id": "Rsj7spvwOXHo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_wine\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.multiclass import OneVsRestClassifier, OneVsOneClassifier\n",
        "\n",
        "# Step 1: Load the Wine dataset\n",
        "data = load_wine()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Step 2: Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Step 3: Train SVM with One-vs-Rest (OvR) strategy\n",
        "ovr_classifier = OneVsRestClassifier(SVC(kernel='rbf', random_state=42))\n",
        "ovr_classifier.fit(X_train, y_train)\n",
        "\n",
        "# Step 4: Make predictions and evaluate accuracy for OvR\n",
        "y_pred_ovr = ovr_classifier.predict(X_test)\n",
        "accuracy_ovr = accuracy_score(y_test, y_pred_ovr)\n",
        "\n",
        "# Step 5: Train SVM with One-vs-One (OvO) strategy\n",
        "ovo_classifier = OneVsOneClassifier(SVC(kernel='rbf', random_state=42))\n",
        "ovo_classifier.fit(X_train, y_train)\n",
        "\n",
        "# Step 6: Make predictions and evaluate accuracy for OvO\n",
        "y_pred_ovo = ovo_classifier.predict(X_test)\n",
        "accuracy_ovo = accuracy_score(y_test, y_pred_ovo)\n",
        "\n",
        "# Step 7: Compare and print the results\n",
        "print(f\"Accuracy with One-vs-Rest (OvR): {accuracy_ovr:.2f}\")\n",
        "print(f\"Accuracy with One-vs-One (OvO): {accuracy_ovo:.2f}\")\n"
      ],
      "metadata": {
        "id": "cWQACf19Oofd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q17.** Write a Python program to train an SVM Classifier using Linear, Polynomial, and RBF kernels on the Breast\n",
        "Cancer dataset and compare their accuracy"
      ],
      "metadata": {
        "id": "NIF4UZj4OpLQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Step 1: Load the Breast Cancer dataset\n",
        "data = load_breast_cancer()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Step 2: Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Step 3: Train SVM Classifier with Linear Kernel\n",
        "svm_linear = SVC(kernel='linear', random_state=42)\n",
        "svm_linear.fit(X_train, y_train)\n",
        "\n",
        "# Step 4: Train SVM Classifier with Polynomial Kernel\n",
        "svm_poly = SVC(kernel='poly', degree=3, random_state=42)  # degree=3 for cubic polynomial kernel\n",
        "svm_poly.fit(X_train, y_train)\n",
        "\n",
        "# Step 5: Train SVM Classifier with RBF Kernel\n",
        "svm_rbf = SVC(kernel='rbf', random_state=42)\n",
        "svm_rbf.fit(X_train, y_train)\n",
        "\n",
        "# Step 6: Make predictions and evaluate accuracy for Linear Kernel\n",
        "y_pred_linear = svm_linear.predict(X_test)\n",
        "accuracy_linear = accuracy_score(y_test, y_pred_linear)\n",
        "\n",
        "# Step 7: Make predictions and evaluate accuracy for Polynomial Kernel\n",
        "y_pred_poly = svm_poly.predict(X_test)\n",
        "accuracy_poly = accuracy_score(y_test, y_pred_poly)\n",
        "\n",
        "# Step 8: Make predictions and evaluate accuracy for RBF Kernel\n",
        "y_pred_rbf = svm_rbf.predict(X_test)\n",
        "accuracy_rbf = accuracy_score(y_test, y_pred_rbf)\n",
        "\n",
        "# Step 9: Compare the results\n",
        "print(f\"Accuracy with Linear Kernel: {accuracy_linear:.2f}\")\n",
        "print(f\"Accuracy with Polynomial Kernel: {accuracy_poly:.2f}\")\n",
        "print(f\"Accuracy with RBF Kernel: {accuracy_rbf:.2f}\")\n"
      ],
      "metadata": {
        "id": "_2E3qT5yPDCV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q18.** Write a Python program to train an SVM Classifier using Stratified K-Fold Cross-Validation and compute the\n",
        "average accuracy"
      ],
      "metadata": {
        "id": "YziQ5CHTPDi4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score\n",
        "import numpy as np\n",
        "\n",
        "# Step 1: Load the Breast Cancer dataset\n",
        "data = load_breast_cancer()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Step 2: Set up Stratified K-Fold Cross-Validation\n",
        "kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "# Step 3: Initialize SVM Classifier\n",
        "svm_classifier = SVC(kernel='rbf', random_state=42)\n",
        "\n",
        "# Step 4: Store accuracy for each fold\n",
        "accuracies = []\n",
        "\n",
        "# Step 5: Perform Stratified K-Fold Cross-Validation\n",
        "for train_index, test_index in kfold.split(X, y):\n",
        "    # Split data into training and testing sets for this fold\n",
        "    X_train, X_test = X[train_index], X[test_index]\n",
        "    y_train, y_test = y[train_index], y[test_index]\n",
        "\n",
        "    # Train the model\n",
        "    svm_classifier.fit(X_train, y_train)\n",
        "\n",
        "    # Predict on the test set\n",
        "    y_pred = svm_classifier.predict(X_test)\n",
        "\n",
        "    # Compute accuracy for this fold\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    accuracies.append(accuracy)\n",
        "\n",
        "# Step 6: Compute the average accuracy\n",
        "average_accuracy = np.mean(accuracies)\n",
        "\n",
        "# Step 7: Print the result\n",
        "print(f\"Accuracy for each fold: {accuracies}\")\n",
        "print(f\"Average Accuracy: {average_accuracy:.2f}\")\n"
      ],
      "metadata": {
        "id": "cz-MNV1vPKL1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q19.**Write a Python program to train a Naïve Bayes classifier using different prior probabilities and compare\n",
        "performance"
      ],
      "metadata": {
        "id": "sRNN5y8SPKlZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Step 1: Load the Breast Cancer dataset\n",
        "data = load_breast_cancer()\n",
        "X = data.data\n",
        "y = data.target  # 0 = malignant, 1 = benign\n",
        "\n",
        "# Step 2: Split into training and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Step 3: Define different prior probability settings\n",
        "# Actual class distribution (as a reference)\n",
        "class_0_ratio = sum(y_train == 0) / len(y_train)\n",
        "class_1_ratio = sum(y_train == 1) / len(y_train)\n",
        "\n",
        "priors_list = [\n",
        "    None,  # Let the model learn priors from data\n",
        "    [0.5, 0.5],  # Uniform priors\n",
        "    [0.3, 0.7],  # Custom skewed priors (favors class 1)\n",
        "    [0.7, 0.3],  # Custom skewed priors (favors class 0)\n",
        "]\n",
        "\n",
        "# Step 4: Evaluate each model\n",
        "for priors in priors_list:\n",
        "    nb = GaussianNB(priors=priors)\n",
        "    nb.fit(X_train, y_train)\n",
        "    y_pred = nb.predict(X_test)\n",
        "    acc = accuracy_score(y_test, y_pred)\n",
        "    print(f\"Prior: {priors if priors else 'learned from data'} --> Accuracy: {acc:.4f}\")\n"
      ],
      "metadata": {
        "id": "wMyQmnhFPWiy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q20.**Write a Python program to perform Recursive Feature Elimination (RFE) before training an SVM Classifier and\n",
        "compare accuracy"
      ],
      "metadata": {
        "id": "tUOYHYyFPXEV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.feature_selection import RFE\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Step 1: Load dataset\n",
        "data = load_breast_cancer()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Step 2: Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Step 3: Train SVM on all features\n",
        "svm_full = SVC(kernel='linear', random_state=42)\n",
        "svm_full.fit(X_train, y_train)\n",
        "y_pred_full = svm_full.predict(X_test)\n",
        "accuracy_full = accuracy_score(y_test, y_pred_full)\n",
        "\n",
        "# Step 4: Apply RFE for feature selection\n",
        "# Select top 10 features (you can change this number)\n",
        "rfe = RFE(estimator=SVC(kernel='linear'), n_features_\n"
      ],
      "metadata": {
        "id": "Q3MCfhVmPfXc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q21.**Write a Python program to train an SVM Classifier and evaluate its performance using Precision, Recall, and\n",
        "F1-Score instead of accuracy"
      ],
      "metadata": {
        "id": "CEwKInaLPfyU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "# Step 1: Load the Breast Cancer dataset\n",
        "data = load_breast_cancer()\n",
        "X = data.data\n",
        "y = data.target  # Binary: 0 = malignant, 1 = benign\n",
        "\n",
        "# Step 2: Split dataset into train and test\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Step 3: Train SVM Classifier\n",
        "svm = SVC(kernel='rbf', random_state=42)\n",
        "svm.fit(X_train, y_train)\n",
        "\n",
        "# Step 4: Predict on test set\n",
        "y_pred = svm.predict(X_test)\n",
        "\n",
        "# Step 5: Evaluate with precision, recall, and F1-score\n",
        "print(\"Classification Report:\")\n",
        "print(classification_report(y_test, y_pred, target_names=data.target_names))\n"
      ],
      "metadata": {
        "id": "aclVB12nPlkL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q22.**Write a Python program to train a Naïve Bayes Classifier and evaluate its performance using Log Loss\n",
        "(Cross-Entropy Loss)"
      ],
      "metadata": {
        "id": "lzGVqb_aPmiq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.metrics import log_loss\n",
        "\n",
        "# Step 1: Load the Breast Cancer dataset\n",
        "data = load_breast_cancer()\n",
        "X = data.data\n",
        "y = data.target  # 0 = malignant, 1 = benign\n",
        "\n",
        "# Step 2: Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Step 3: Train Naï\n"
      ],
      "metadata": {
        "id": "rDVFa5UBPtAu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q23.**Write a Python program to train an SVM Classifier and visualize the Confusion Matrix using seaborn"
      ],
      "metadata": {
        "id": "XzY20KhDPtd9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
        "\n",
        "# Step 1: Load the Breast Cancer dataset\n",
        "data = load_breast_cancer()\n",
        "X = data.data\n",
        "y = data.target  # 0 = malignant, 1 = benign\n",
        "\n",
        "# Step 2: Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Step 3: Train SVM Classifier\n",
        "svm = SVC(kernel='rbf', random_state=42)\n",
        "svm.fit(X_train, y_train)\n",
        "\n",
        "# Step 4: Make predictions\n",
        "y_pred = svm.predict(X_test)\n",
        "\n",
        "# Step 5: Compute confusion matrix\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "# Step 6: Visualize with seaborn\n",
        "plt.figure(figsize=(6, 4))\n",
        "sns.heatmap(cm\n"
      ],
      "metadata": {
        "id": "dWV3s1nLPzY0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q24.**Write a Python program to train an SVM Regressor (SVR) and evaluate its performance using Mean Absolute\n",
        "Error (MAE) instead of MSE"
      ],
      "metadata": {
        "id": "2X4MqP8pPz1n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVR\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "\n",
        "# Step 1: Load California Housing dataset\n",
        "data = fetch_california_housing()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Step 2: Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Step 3: Feature scaling (important for SVR)\n",
        "scaler_X = StandardScaler()\n",
        "scaler_y = StandardScaler()\n",
        "\n",
        "X_train_scaled = scaler_X.fit_transform(X_train)\n",
        "X_test_scaled = scaler_X.transform(X_test)\n",
        "\n",
        "y_train_scaled = scaler_y.fit_transform(y_train.reshape(-1, 1)).ravel()\n",
        "\n",
        "# Step 4: Train SVR model\n",
        "svr = SVR(kernel='rbf')\n",
        "svr.fit(X_train_scaled, y_train_scaled)\n",
        "\n",
        "# Step 5: Predict and inverse transform predictions\n",
        "y_pred_scaled = svr.predict(X_test_scaled)\n",
        "y_pred = scaler_y.inverse_transform(y_pred_scaled.reshape(-1, 1)).ravel()\n",
        "\n",
        "# Step 6: Evaluate using Mean Absolute Error\n",
        "mae = mean_absolute_error(y_test, y_pred)\n",
        "print(f\"Mean Absolute Error (MAE): {mae:.4f}\")\n"
      ],
      "metadata": {
        "id": "21QncXb5P7WQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q25.**Write a Python program to train a Naïve Bayes classifier and evaluate its performance using the ROC-AUC\n",
        "score"
      ],
      "metadata": {
        "id": "7ZDvCQpiP7yW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.metrics import roc_auc_score, roc_curve\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Step 1: Load the Breast Cancer dataset\n",
        "data = load_breast_cancer()\n",
        "X = data.data\n",
        "y = data.target  # Binary labels: 0 = malignant, 1 = benign\n",
        "\n",
        "# Step 2: Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Step 3: Train Naïve Bayes classifier\n",
        "nb = GaussianNB()\n",
        "nb.fit(X_train, y_train)\n",
        "\n",
        "# Step 4: Predict probabilities\n",
        "y_prob = nb.predict_proba(X_test)[:, 1]  # Probability for class 1 (benign)\n",
        "\n",
        "# Step 5: Compute ROC-AUC score\n",
        "roc_auc = roc_auc_score(y_test, y_prob)\n",
        "print(f\"ROC-AUC Score: {roc_auc:.4f}\")\n",
        "\n",
        "# Step 6: Plot ROC curve\n",
        "fpr, tpr, thresholds = roc_curve(y_test, y_prob)\n",
        "plt.figure(figsize=(6, 4))\n",
        "plt.plot(fpr, tpr, label=f'Naïve Bayes (AUC = {roc_auc:.2f})', color='darkorange')\n",
        "plt.plot([0, 1], [0, 1], linestyle='--', color='gray')\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.title('ROC Curve - Naïve Bayes Classifier')\n",
        "plt.legend(loc='lower right')\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "78AMuhd8QG-z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q26.**Write a Python program to train an SVM Classifier and visualize the Precision-Recall Curve."
      ],
      "metadata": {
        "id": "IxVpM65hQHbj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import precision_recall_curve, average_precision_score\n",
        "\n",
        "# Step 1: Load the dataset\n",
        "data = load_breast_cancer()\n",
        "X = data.data\n",
        "y = data.target  # 0 = malignant, 1 = benign\n",
        "\n",
        "# Step 2: Split the data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Step 3: Train an SVM with probability estimates enabled\n",
        "svm = SVC(kernel='rbf', probability=True, random_state=42)\n",
        "svm.fit(X_train, y_train)\n",
        "\n",
        "# Step 4: Predict probabilities\n",
        "y_scores = svm.predict_proba(X_test)[:, 1]  # Probability for class 1 (benign)\n",
        "\n",
        "# Step 5: Compute precision-recall pairs\n",
        "precision, recall, _ = precision_recall_curve(y_test, y_scores)\n",
        "avg_precision = average_precision_score(y_test, y_scores)\n",
        "\n",
        "# Step 6: Plot the Precision-Recall Curve\n",
        "plt.figure(figsize=(6, 4))\n",
        "plt.plot(recall, precision, color='blue', label=f'AP = {avg_precision:.2f}')\n",
        "plt.xlabel('Recall')\n",
        "plt.ylabel('Precision')\n",
        "plt.title('Precision-Recall Curve - SVM Classifier')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "T3R3HUueQOnQ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}